大家好，今天这个讲座给大家介绍一下TiDB整体架构。我是PingCAP的技术VP申砾。

今天这门课程会从这样几方面进行介绍，包括：TiDB是什么，它是一个什么样的数据库，它的整体的架构，它的分层的情况，以及它最重要的两层，一个是我们的分布式存储引擎TiKV，另外一个就是分布式SQL引擎TiDB。介绍这样几个要点。

首先我们来看TiDB是什么。一句话来说，TiDB是一个分布式、强一致、具备水平扩展能力的关系型数据库。整个数据库是由我们PingCAP公司自主研发，完全开源的这样一个数据库。我们在设计这个数据库之初，聚焦在这四个特性。

第一个：水平扩展。作为一个分布式数据库来说，水平扩展是一个最基本最基本的能力。当你的整个数据库的能力不够，我们可以通过简单的加机器，去帮你把无论是存储的能力还是计算的能力进行水平扩展。

第二点：高可用。TiDB作为分布式数据库来说，它的集群中的节点可能会非常多，可能有几十个甚至上百个节点。那么这些节点的异常情况的失效，或者是正常情况下我们对集群做滚动升级，我们都要解决少量节点失效的高可用问题。当然我们还能去做跨数据中心的高可用，比如说一个数据中心挂掉之后，还有另外的数据中心能够提供服务。

第三点：ACID事务。其实很多分布式的存储系统比如说HBase，它们通过放弃SQL，放弃事务来获得更简单的存储模型，来实现更好的水平扩展能力。但其实在严肃的生产环境中，特别是一些交易类的核心产品中，事务是非常重要的一个特性。所以大家会看到很多在HBase基础上做事务的解决方案。所以我们也提供了事务支持，而且我们认为事务这一层如果存储不解决，那么外面的业务或中间件就要去解决，而通过外部业务或中间件去解决很难做得高效。

最后一点：SQL。我们最终是提供一个关系型数据库，我们希望提供完整的SQL支持，让用户的业务写起来尽可能简单。

这张图是TiDB的整体架构，或者说是TiDB家族的整体架构，里面不只包括TiDB数据库的内核，也包括数据库的一些周边工具，比如导入导出工具，Binlog向下游同步的工具，整个运维，还有大数据解决方案TiSpark，以及监控组件Prometheus，等等，是一个很复杂的系统。作为这么复杂的一个分布式系统，除了数据库内核之外还有这么多的组件，如何简化它们的运维，我们提供两种方式：第一种方式，通过Ansible的方式，直接在裸机上部署整套服务，也能部署周边工具。

另外我们也提供了Kubernetes支持，我们希望通过Kubernetes这样一套弹性扩展的，管理下面整个物理资源的方案，能够简化分布式数据库的运维和使用，这就是我们Kubernetes方案的核心架构。我们在Kubernetes中实现了自己的Operator，帮助Kubernetes理解如何管理、调度、运维这样一个复杂的分布式数据库，后面会有专门的课程进行介绍。

回到最核心的数据库内核，整个架构就会简单很多。可以看到，数据库内核分了两层关键组件，第一层是TiDB，是一个无状态的计算引擎，它来和外面的应用客户端做交互，通过MySQL协议接收外面的请求，对外面的应用来说，看到的就是MySQL。下面是我们的TiKV，分布式带事务的Key-Value存储引擎，它真正地把数据存进去，进行存储的水平扩展，以及接收由上面SQL引擎发下来的计算请求，从本地拿取结果之后，再把计算结果返回给上层。最右边是PD，是我们的集群管理节点（Placement Driver），它主要是存储TiKV集群的元信息，就是每一片数据到底在哪里，以及对整个TiKV集群的负载进行调度。我们可以简单的理解为，PD和TiKV构成了一个分布式、能够水平扩展的Key-Value存储引擎，TiDB是整个数据库的计算层，这样一个两层的角度来看。后面的介绍也会以这样的两层来进行。

首先我们看TiDB的存储引擎，也就是TiKV集群。我们会了解到这个存储引擎是怎么做，包括它每一层是如何实现的，它的数据最终存在RocksDB中，上面通过Raft进行水平扩展和数据的强一致复制。然后我们还会介绍如何对数据库进行自动的水平扩展，比如加一个空节点之后，如何将现有节点的数据均衡过去。还有数据库的调度和分布式事务。

首先来看TiKV分布式存储引擎的整体架构，这是一个高度分层的架构，主要分为这四层：

最下面一层是Local KV Storage，是本地的存储引擎，我们用的是RocksDB，它是由Facebook开发的嵌入式KV存储引擎，速度非常快，而且有一些非常好的特性，如原子Batch Write，多线程Compaction。RocksDB会帮助TiKV把数据存在磁盘上，但它并不能保证高可用，所以我们在上面还做了Raft（当单个节点失效时还有其他的强一致副本），更上层还有MVCC层和事务层，提供更高级的功能。

大家可以看到，这是一个高度分层的架构，每一层和每一层之间的耦合比较薄，这样做的好处是，一方面我们可以对每一层进行单独的优化，让每一层保证对外接口语义是不变的，在每一层内部做针对性优化。另一方面也可以把单独某一层抽掉，换上其他的组件，或者是换上Mock组件，利于做测试。另外一个特点是我们依赖于本地文件系统，不依赖于分布式文件系统，这样整个系统的延迟会更低。

首先我们看数据是如何落地和容灾的。我们的数据是存储在RocksDB实例中，但是存到RocksDB实例中，它是帮你把数据落到硬盘上，但是当这块硬盘坏掉，或者整个机器坏掉时，这部分数据也就丢掉了。所以RocksDB帮我们解决了数据落地问题，并没有解决数据的高可用问题，也就是容灾。要进行容灾，就一定要有多副本，当一个副本损失掉之后，还有其他副本继续提供服务。那么要做多副本，就要选一种副本之间的数据复制方案，而且如果要实现更好的容灾，可能需要强一致复制，就是一个副本丢掉之后，数据还是不会丢，还是一致的，对外表现出强一致性。我们选择Raft，通过Raft在多个副本之间做强一致且高效的复制。Raft是非常核心的一个模块，它有几个特性，后面我们会用到，这里简单提一下。第一，它是一个多数派写入的协议，假设Raft将数据复制三份，那么每次写入至少保证两个副本写入成功，才能客户端返回成功，这样即使任何一个副本丢掉，都能保证有一个副本是有最新数据的，所以数据是不会丢的，所以能实现强一致的复制。第二，它是一个强Leader的协议，数据的写入和读取都是通过Leader进行，多个副本只有一个是Leader，剩下两个是Follower，通过Leader进行读写是Raft的一个约束。当然我们后面还会做更多的优化，比如一些读请求可以通过Follower进行，去减少延迟。Leader和Follower角色的位置不是固定的，比如说有三个副本，谁当Leader，谁当Follower，是动态进行的。Raft有一套成员选举的算法，这套算法保证Leader失效的情况下，Follower能自动进行选举，选出一个有最新数据的节点当作新的Leader继续提供服务。所以通过Raft我们能实现数据的容灾和高可用。单个副本失效之后，会有新的Leader继续对外提供服务。刚才提到的成员变更，Leader，Follower角色的变换，无论是异常情况的变换比如说一个副本Leader丢掉了，Follower会自动选举成功，或者说我们通过一些调度的方式，主动进行Leader，Follower角色的迁移，加一个新的副本或者删除一个副本，这些都是Raft成员变更的一部分。后面的数据负载均衡，以及数据副本的约束，包括三副本变更为五副本，或者三副本丢失一个副本，都是通过这套成员变更算法来补齐数据以及实现负载均衡。有了Raft之后，我们就可以将数据在不同的机器、机架、数据中心之间进行分布以及复制，这是一个非常非常核心的概念。

这里我们只是说一份数据如何有多个副本，我们如何去将整个一份大量的数据打散在一个多台机器的集群上，这是第二个问题。Raft解决的是一份数据如何做强一致的多副本。下面我们要看如何做真正的分布式。做分布式的第一个决策就是如何将数据进行切片，只有进行切片，才有可能将不同切片的副本调度在不同的位置上，实现在集群上的分布。要进行分片，就要选择一个数据分片算法，常用的有两种，一种是基于Hash的分片，就是说拿到一个Key计算Hash值，然后决定将它放到哪个分片上。但是这样的话很难做一段范围Key的Scan，因为被打散在整个集群上了。但是，因为我们最终是想做一个SQL数据库，一定要支持连续范围的Scan，比如说通过索引获取一段数据，或者扫描全表数据，所以我们选择的是基于Range的分片。我们将TiKV中的Key的逻辑空间看成一个有序的Map，从无穷小到无穷大，有这样两个特殊的Key-Value。我们将这个Key空间进行切分，切成一段一段的，每一段有一个元信息也就是起始和结束的Key。这里的Key和Value都是byte数组，Key的有序是指Key按照byte数组的比特位比较是有序的。

这个切片是如何划分的呢？我们是根据一个Region里面，所有Key、Value的大小加起来得到的字节数，以这个为分片的依据。当一个Region的数据超过某个阈值时（默认是96MB)，就会进行自动的切分，变成多个分片。

这是一个例子。首先假设我们有一个Region，可能集群一开始比较空，只有一个Region。随着数据不断写入，Region中的数据不断变多，这个时候就会进行自动的拆分，将数据从一个Region拆成两个Region。注意这里是元信息的拆分而不是真实物理信息的拆分。这里的元信息是指Region的Start Key、End Key这样的Meta Data。这两个Region中的数据还是存在一个TiKV实例中的RocksDB中，物理上还是存在一起，只是在逻辑上变成两个Region进行管理。只有在逻辑上将它变成不同的元信息进行管理，下一步才可能在物理上把它挪到不同的节点上，实现物理上的分离和调度。整个分裂是在自动地、不断地进行，DBA不需要做任何操作。

分裂之后我们怎么实现水平扩展？分裂之后数据还是在一台机器上，只是通过两个元信息进行管理。这里是个例子，比如有这样一个集群，有4个节点，其中有一个节点数据量比较多，存了3块数据，可能快满了。我们加一个空的节点进去，看看集群如何实现水平扩展。

假设集群决定将Region 1的副本调度到新的节点上去，然后将它删掉，这样就实现了数据的水平扩展。第一步，将这个副本的Leader角色迁移走，因为读写都走Leader，把它调度走之后，后续的迁移就不会影响线上的业务。第二步，通过Raft加副本命令增加一个副本，从Leader拉这一片数据的全量镜像，再用当时拉全量镜像时的Raft Log偏移量去拉后面的增量，然后再把增量追上。

下一步，我们再把这个副本删除掉。在物理上是通过加副本和删副本这两个命令实现逻辑上的数据迁移，整个过程是在动态不断的进行。

整个负载均衡的过程是谁来控制的？是由集群中PD（Placement Driver）角色来进行控制。它存储了整个集群的元信息，就是它知道整个集群的状态，整个集群有多少Region，每个Region有多少副本，分别在哪里，谁是Leader。所有的TiKV节点以及所有的Region Leader都会不断地通过心跳消息向PD汇报自己最新的状态，PD就能获取全局的信息，就是整个集群是什么样的状态，每个节点上存储空间剩余多少，CPU使用率，内存使用量，读写流量等等信息。有了这些全局信息之后，PD再根据管理员配置的策略，这个策略可能是副本的数量限制，比如说每一块数据有几个副本，可能调度速率（？）的限制，比如说进行Leader迁移，比如说加副本和删副本的速度有没有上限，因为所有的调度迁移都是有代价开销的，像CPU、网络、内存等等这些资源。为了避免对线上产生影响，我们有一个速度的限制，当然我们的默认限制是比较保守的，如果不是线上的话，可以把速度调高。PD一方面根据集群的元信息，一方面根据管理员配置的策略，生成调度命令，下发到TiKV集群，然后对整个集群的负载均衡进行调度。所以PD是整个集群的大脑。

前面提到了Raft，PD，负载均衡，水平扩展，我们看到了如何做一个分布式的Key-Value存储引擎，能够自己做负载均衡，能够扩展到多台机器上，最终我们要提供事务支持，是在TiKV这个存储引擎层做的。我们的事务模型是Google Percolator，这是一个分布式事务的解决方案，是一个基本上去中心化的两阶段提交。为什么我们说它是基本上去中心化呢？因为我们还有一个单点，就是事务的TSO，每个事务会生成一个版本号，这个版本号是由一个中心节点来生成的，这个中心节点就是PD集群，PD集群的Leader会生成时间戳，当PD的Leader出现问题的时候，Follower会自动变成Leader，依然能够对外提供TSO的服务。而且整个时间戳的服务是比较轻量级的操作，对性能的影响并不大。其他的如事务管理器、锁、这些状态全都是打散在整个集群上。

另外提一点，3.0之前的事务的锁模型只支持乐观锁，我们不会在事务执行过程中真正地上锁，而是在事务提交过程中去检查锁的冲突，所以是一种乐观锁的模型。这在冲突比较高的场景中并不太适用。我们在TiDB 3.0中提供了悲观锁的特性，类似MySQL/Oracle，在语句运行期间就对数据上锁，保证锁住的数据在提交时大概率能提交成功。现在的默认隔离级别是Snapshot Isolation，目前只有（？）这种隔离级别。这篇文档是Google Percolator的实现，大家有兴趣可以看一下。

到现在我们就有了一个分布式、带事务的Key-Value存储引擎，整个架构是这样的：有很多TiKV实例，每个实例上会存放一些Region的副本，不同实例上的多个副本构成Raft Group，相互之间进行数据的复制，通过Leader写入数据，Leader再复制给Follower。PD是掌握整个集群元信息，对整个集群进行负载均衡调度，同时外面的客户端访问TiKV集群的时候，第一次访问时会跟PD查询我要访问的Key到底在哪里，它的路由信息，也就是这个Key所对应的Region是谁，Region Leader在哪里。第二次访问就会把这个路由信息缓存住，就不会每次访问TiKV集群都向PD要路由信息。

这时我们已经有了这样一个分布式、带事务的Key-Value的存储引擎。下面我们看一下，如何在这样一个Key-Value存储引擎上做一个SQL引擎。我们会了解整个SQL引擎的架构，一个Query处理的核心流程是什么样子，以及整个分布式计算的框架。

要在一个分布式的KV引擎上做关系型模型，我们要做的第一步就是想一下如何将关系型模型中的一些东西转换成Key-Value，比如说如何将Table，如何将数据，如何将索引转换成Key-Value存储进去。另外我们还提供全局索引，能使用全局一致性的索引，这是很多业务所依赖的。另外因为我们要做一个真正的让业务便于使用的数据库，所以我们的SQL功能需要做的非常完善，特别是我们还提供MySQL协议的兼容，除了标准的SQL功能之外，我们还提供很多和MySQL方言相关的一些SQL功能。最后，因为是分布式数据库，要存储海量数据，那么用户将数据存进来之后，也希望直接在数据库里面进行数据的使用，所以除了简单的OLTP负载之外，我们也希望支持在大数据量上进行轻量级分析这样一种负载，所以我们是HTAP数据库。我们的SQL引擎就需要支持这样一些特性。

这是SQL引擎的架构。这边是协议层，和外面的客户端用MySQL协议通讯，做协议的解析，包括编码解码，拿到协议中的请求。然后再传递到SQL核心层，这一层是真正处理SQL逻辑，包括做语法解析、验证、优化、执行。当然这里面还包括权限、Schema管理、统计信息、各种后台任务。这是我们完全实现的一个SQL引擎，它对MySQL协议兼容是非常优秀的。

那么一条SQL在SQL核心层需要经过哪些处理？我们拿到一条SQL的文本之后，首先通过语法解析，通过Parser将一条SQL文本转成结构化数据：抽象语法树AST。得到AST之后，我们再将这棵语法树进行语义校验、合法性验证等工作，然后传递给优化器。第一步经过逻辑优化，就是经过数学上的等价变换，使得SQL在变换前后的语义等价，但代价计算更小。比如说SELECT * FROM t WHERE 1==0，那么1==0的条件就是永假式，直接转成SELECT * FROM （？？？），这样就不需要任何数据访问。这是一个逻辑优化的例子。进行逻辑优化之后，会进行物理优化，就是考虑整个集群的数据分布，然后决定到底用哪一个（？？？），哪一个物理算子来处理这条Query。这里会参考统计信息，然后来估算每一条访问路径的成本，生成物理计划，再将这些物理计划传递到分布式计算引擎中（执行层）进行执行。

这里举个例子。比如说这样一个简单的Query，就是在全表上算一个WHERE条件，然后计算COUNT。假设这个条件没有索引，需要全表扫描。首先TiDB会根据表结构到Key-Value编码规则，拿到到底哪些数据存在哪些相关节点上，然后再将这些Filter条件以及聚合表达式下推到数据所涉及的那些节点上，这些节点再将数据从本地拿出来进行计算，计算完成后只需将最终结果返回给SQL引擎，SQL引擎会进行数据的汇总。在这个例子中，就是每一个涉及到这个数据的Region，把数据拿出来之后，如果能经过这个Filter，就会把本地的计数器加1，这样每个节点向上返回一个本地的COUNT，最终在TiDB这个SQL引擎进行SUM就能得到最终的结果。这是一个比较简单的例子。

另外TiDB要处理海量数据，其实很多的物理算子都是并行的，因为数据量大的时候，我们利用单机多核的优势，能够很明显地加速数据的计算。比如以这个简单的例子来说，有这样一个Join，Join的一边是全表扫描，一边是读索引，然后再进行Hash Join。那么Table Scang和Index Scan这样两个算子都是并行的，会用多线程去TiKV拿数据，做计算下推，拿到计算结果后再交给Join算子，Join算子也是并行的，会启动多个线程同时做Join，因为数据量大时计算工作量是非常大的。通过单机的多线程能够明显地加速整个计算流程。

对分布式数据库来说，做DDL是一个比较有挑战的事情。为什么？假设大家想一下，我们通过锁表的方式去给一个超大表加一列或者加一个索引，那么线上业务整个就会停止，表越大停止时间越长。所以我们要解决的最核心问题是如何在不影响线上业务的情况下，在分布式数据库上进行Schema变更。我们的整个算法是受Google F1的Schema变更算法启发，是一个在线变更算法，也就是说任何DDL操作都不会锁表，不会阻塞线上业务。比如说可以先加一列（添加列，删除列的速度很快），比如说ADD INDEX操作，索引加完之后再把业务更新上去，整个不会Block线上业务。

后续的相关信息大家可以去这个网站上或者Follow这个公众号来获取更多的信息，好，感谢大家。
